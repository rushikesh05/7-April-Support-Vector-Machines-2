{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
        "\n",
        "##Ans:---\n",
        "\n",
        "###Polynomial functions and kernel functions are both commonly used in machine learning algorithms. Polynomial functions are a type of mathematical function that involve raising a variable to a power, while kernel functions are a way to transform data into a higher-dimensional space to make it easier to separate.\n",
        "\n",
        "###In the context of machine learning, kernel functions are often used to define similarity measures between data points in a high-dimensional feature space. Polynomial functions can also be used to transform data into a higher-dimensional space, and can be used as kernel functions in some machine learning algorithms.\n",
        "\n",
        "###For example, the polynomial kernel is a commonly used kernel function in support vector machines (SVMs) for classification problems. It takes the form of a polynomial function of the dot product between two vectors, and is used to map the data into a higher-dimensional space where it may be easier to separate the classes.\n",
        "\n",
        "###In general, kernel functions provide a way to implicitly represent the data in a higher-dimensional space, without actually having to compute the coordinates of the data in that space. Polynomial functions can be used as kernel functions to achieve this goal, as can other functions such as radial basis functions (RBFs) and sigmoid functions.\n"
      ],
      "metadata": {
        "id": "tzQP5Jp5ACI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
        "\n",
        "##Ans--\n",
        "\n",
        "###Import the necessary libraries:\n"
      ],
      "metadata": {
        "id": "ikenq5E0A8ru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKwJTe6Z955x"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load the dataset:"
      ],
      "metadata": {
        "id": "U48NVFIfBQcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# get the input and output data\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "2zCyEj53BN7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "5J3b-px2BW3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an SVM object with a polynomial kernel\n",
        "svm = SVC(kernel='poly', degree=3)\n"
      ],
      "metadata": {
        "id": "rv5kLHuqBcwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the SVM on the training data\n",
        "svm.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "IoizRQkuBfgv",
        "outputId": "d4755012-3461-4fed-d8e4-4e5e6cadae51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='poly')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the classes of the test data\n",
        "y_pred = svm.predict(X_test)\n"
      ],
      "metadata": {
        "id": "_MfQze4UBidV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TlBs-uUBktE",
        "outputId": "5dd76179-d0f7-4c14-c8b6-a26736f40646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The kernel parameter is set to 'poly' to use a polynomial kernel, and the degree parameter is set to 3 to use a cubic polynomial. You can adjust the value of degree to use a different degree polynomial."
      ],
      "metadata": {
        "id": "JyfzGQYUBV45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
        "\n",
        "##Ans:--\n",
        "\n",
        "###In Support Vector Regression (SVR), epsilon is a hyperparameter that controls the width of the margin allowed for errors between the predicted values and the actual values. The value of epsilon determines the tolerance level for errors, and increasing its value allows for more errors to be allowed within the margin.\n",
        "\n",
        "###When the value of epsilon is increased in SVR, the margin around the predicted values is widened, and more data points may fall within the margin. As a result, the number of support vectors, which are the data points closest to the margin, may increase.\n",
        "\n",
        "###However, the exact effect of changing the value of epsilon on the number of support vectors can vary depending on the specific data set and the values of other hyperparameters used in the SVR model. In general, increasing epsilon will tend to allow for more errors and a wider margin, which may result in more support vectors."
      ],
      "metadata": {
        "id": "jP4_AMx0BuoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
        "\n",
        "##Ans:---\n",
        "\n",
        "\n",
        "\n",
        "###The performance of Support Vector Regression (SVR) depends on several hyperparameters, including the kernel function, C parameter, epsilon parameter, and gamma parameter. Here's a brief overview of how each of these parameters works and how they can affect the performance of an SVR model:\n",
        "\n",
        "###Kernel function: The kernel function is used to transform the input data into a higher-dimensional feature space where it is easier to separate. Commonly used kernel functions include linear, polynomial, and radial basis function (RBF). The choice of kernel function can greatly affect the performance of an SVR model, and it largely depends on the specific characteristics of the data. For example, if the data is highly nonlinear, an RBF kernel may be a good choice, while a linear kernel may work better for more linear data.\n",
        "\n",
        "###C parameter: The C parameter controls the trade-off between achieving a low training error and a low testing error. A small value of C will result in a wider margin, allowing for more errors but reducing the risk of overfitting, while a large value of C will result in a smaller margin, potentially leading to overfitting. In general, a larger C value can help improve the accuracy of the model, but it may also increase the risk of overfitting.\n",
        "\n",
        "###Epsilon parameter: The epsilon parameter determines the width of the margin around the predicted values, allowing for a certain degree of error between the predicted and actual values. A larger value of epsilon will result in a wider margin, allowing for more errors, while a smaller value of epsilon will result in a narrower margin, requiring the model to be more precise. The choice of epsilon depends on the specific application, and it should be chosen to balance the trade-off between accuracy and robustness.\n",
        "\n",
        "###Gamma parameter: The gamma parameter controls the shape of the kernel function and affects how much each data point influences the decision boundary. A small value of gamma will result in a smoother decision boundary, while a large value of gamma will result in a more complex, wiggly decision boundary. In general, a smaller gamma value can help improve the generalization performance of the model, but it may also result in lower accuracy.\n",
        "\n",
        "##Here are some examples of when you might want to increase or decrease each of these parameters:\n",
        "\n",
        "* Kernel function: If the data is highly nonlinear, an RBF kernel may be a good choice, while a linear kernel may work better for more linear data.\n",
        "\n",
        "* C parameter: If the training error is high, increasing the C parameter can help reduce the error and improve the accuracy of the model. However, if the model is overfitting, decreasing the C parameter can help reduce the risk of overfitting.\n",
        "\n",
        "*  Epsilon parameter: If the data is noisy or there is a high degree of uncertainty, increasing the epsilon parameter can help improve the robustness of the model. However, if the model needs to be very precise, decreasing the epsilon parameter can help ensure that the model is accurate.\n",
        "\n",
        "* Gamma parameter: If the decision boundary is too complex and wiggly, decreasing the gamma parameter can help smooth out the boundary and improve the generalization performance of the model. However, if the decision boundary is too simple and the model is underfitting, increasing the gamma parameter can help make the boundary more complex and improve the accuracy of the model."
      ],
      "metadata": {
        "id": "cVxYcB-4QksY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q5. Assignment:\n",
        "```\n",
        "Import the necessary libraries and load the dataseg\n",
        "Split the dataset into training and testing setZ\n",
        "Preprocess the data using any technique of your choice (e.g. scaling, normalization\n",
        "Create an instance of the SVC classifier and train it on the training datW\n",
        "hse the trained classifier to predict the labels of the testing datW\n",
        "Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-scoreK\n",
        "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
        "improve its performanc_\n",
        "Train the tuned classifier on the entire dataseg\n",
        "Save the trained classifier to a file for future use.\n",
        "```"
      ],
      "metadata": {
        "id": "yijITVfKRHMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pickle5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4HUl3VFSADw",
        "outputId": "30873437-7bb0-4f25-d965-f11c7bc2e235"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp39-cp39-linux_x86_64.whl size=255890 sha256=2c1f048725ae478d601c9203d892250e7520870c80baaebb2408886e2732da9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/7a/49/9bef8878949914ecb90c08fc5bf30a05e17f475fe7e08b63a8\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create an instance of the SVC classifier and train it on the training data\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Use the trained classifier to predict the labels of the testing data\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the classifier using accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']}\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Train the tuned classifier on the entire dataset\n",
        "svc_tuned = grid_search.best_estimator_\n",
        "svc_tuned.fit(X, y)\n",
        "\n",
        "# Save the trained classifier to a file for future use\n",
        "joblib.dump(svc_tuned, 'svm_classifier.joblib')\n"
      ],
      "metadata": {
        "id": "UEOFU1sDBp9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f219630-8a54-4fe0-f34f-0eff57a8b529"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_classifier.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaYATiw1Rz8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}